from typing import Annotated
from langchain_core.messages import BaseMessage, SystemMessage
from typing_extensions import TypedDict
from typing import TypedDict, Annotated, List, Union, Dict, Any
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
import os
import time
import sys
from pathlib import Path
from langchain_mcp_adapters.client import MultiServerMCPClient
import asyncio

# #######################  use deepseek  #########################
# # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
# DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

# # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰è®¾ç½®APIå¯†é’¥ï¼Œåˆ™æç¤ºç”¨æˆ·æ‰‹åŠ¨è¾“å…¥
# if not DEEPSEEK_API_KEY:
#     print("è­¦å‘Š: æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
#     DEEPSEEK_API_KEY = input("è¯·è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥: ").strip()
#     # è®¾ç½®ç¯å¢ƒå˜é‡
#     os.environ["DEEPSEEK_API_KEY"] = DEEPSEEK_API_KEY

# # Create LLM
# from langchain_deepseek import ChatDeepSeek
# llm = ChatDeepSeek(model="deepseek-chat", api_key=DEEPSEEK_API_KEY)

#######################  use qwen  #########################
# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
QWEN_API_KEY = os.environ.get("QWEN_API_KEY")

# å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰è®¾ç½®APIå¯†é’¥ï¼Œåˆ™æç¤ºç”¨æˆ·æ‰‹åŠ¨è¾“å…¥
if not QWEN_API_KEY:
    print("è­¦å‘Š: æœªè®¾ç½®QWEN_API_KEYç¯å¢ƒå˜é‡")
    QWEN_API_KEY = input("è¯·è¾“å…¥æ‚¨çš„Qwen APIå¯†é’¥: ").strip()
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["QWEN_API_KEY"] = QWEN_API_KEY

# # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨deepseekï¼‰
#é‡‡ç”¨ChatOpenAIè°ƒç”¨ç¤ºä¾‹
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model="qwen3-coder-plus",
    openai_api_key=QWEN_API_KEY,
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0
)

#######################  use other LLMs  #########################
# see https://langchain-ai.github.io/langgraph/agents/models/

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èˆªå¤©å™¨è½¨é“åŠ›å­¦ä¸“å®¶AIåŠ©æ‰‹ï¼Œä½ çš„åå­—å«'å°æ¾é¼ 'ã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä¼˜å…ˆå€Ÿç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼Œå¦‚æœå·¥å…·ä¸é€‚ç”¨ï¼Œå†è€ƒè™‘æ‰‹åŠ¨è®¡ç®—ï¼›
2. å¦‚æœä½ ä¸ç¡®å®šæŸä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°å‘Šè¯‰ç”¨æˆ·ï¼›
3. ç”Ÿæˆçš„æ•°æ®é»˜è®¤ä¿å­˜åœ¨'./files/'é‡Œï¼›
4. è°ƒç”¨å·¥å…·æ—¶ï¼Œæ–‡ä»¶åå¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼›
5. è¯·å…ˆç»™å‡ºæ€è€ƒè¿‡ç¨‹ï¼Œå†è°ƒç”¨å·¥å…·æˆ–å›ç­”ï¼›
"""

class State(TypedDict):
    messages: Annotated[list, add_messages]

def print_welcome():
    """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
    welcome_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                     ğŸš€ è½¨é“ä¸“å®¶ AI åŠ©æ‰‹ ğŸš€                     
                                                                
  æˆ‘æ˜¯æ‚¨çš„è½¨é“ä¸“å®¶åŠ©æ‰‹<å°æ¾é¼ >ï¼Œèƒ½å¤Ÿå¸®åŠ©æ‚¨ï¼š                              
  â€¢ å«æ˜Ÿè½¨é“è®¡ç®—                                             
  â€¢ æ–‡ä»¶æ“ä½œå’Œæ•°æ®å¤„ç†                                           
  â€¢ ...
                                                                
  è¾“å…¥ 'bye' é€€å‡ºç¨‹åº                   
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(welcome_text)

def typewriter_print(text, delay=0.02):
    """æ‰“å­—æœºæ•ˆæœæ‰“å°æ–‡æœ¬"""
    for char in text:
        sys.stdout.write(char)
        sys.stdout.flush()
        time.sleep(delay)
    print()  # æ¢è¡Œ

def print_section_divider():
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "â•" * 60 + "\n")

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.resolve()
# è·å–documentsç›®å½•ï¼ˆç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„Documentsæ–‡ä»¶å¤¹ï¼‰
DOCUMENTS_DIR = Path.home() / "Documents"

# ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
FILES_DIR = PROJECT_ROOT / "files"
FILES_DIR.mkdir(exist_ok=True)

# Initialize async components
async def initialize_tools():
    """å¼‚æ­¥åˆå§‹åŒ–å·¥å…·"""
    client = MultiServerMCPClient(
        {
            "filesystem": {
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-filesystem",
                    str(PROJECT_ROOT),
                    str(DOCUMENTS_DIR)
                ],
                "transport": "stdio"
            },

            "memory": {
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-memory"
                ],
                "env": {
                    "MEMORY_FILE_PATH": str(FILES_DIR / "memory.json")
                },
                "transport": "stdio"
            },


            "sequential-thinking": {
                "command": "npx",
                "args": [
                    "-y",
                    "@modelcontextprotocol/server-sequential-thinking"
                ],
                "transport": "stdio" 
            },

            "mcp-server-satOrbit": {
                "command": "uv",
                "args": [
                    "--directory",
                    "D:/home/projects/mcp-server-satellite-orbit/",  # run_server.pyæ‰€åœ¨çš„è·¯å¾„
                    "run",
                    "run_server.py"
                ],
                "transport": "stdio"
            },
            
            
        }
    )

    # Get MCP tools
    tools_mcp = await client.get_tools()

    # Properly combine tools into a flat list
    tools = []

    # Handle the case where tools_mcp might be a single tool or a list of tools
    if isinstance(tools_mcp, list):
        tools.extend(tools_mcp)
    else:
        tools.append(tools_mcp)

    # Debug: Print tool information
    print(f"Total tools loaded: {len(tools)}")
    for i, tool in enumerate(tools):
        if hasattr(tool, 'name'):
            print(f"  {i+1}. {tool.name}")
        else:
            print(f"  {i+1}. {type(tool).__name__}")

    return tools

# Initialize tools
tools = asyncio.run(initialize_tools())

# bind tools for llm
llm_with_tools = llm.bind_tools(tools)

async def chatbot(state: State):
    """å¼‚æ­¥å¤„ç†å¯¹è¯çš„ä¸»å‡½æ•°ï¼ŒåŒ…å«ç³»ç»Ÿæç¤ºè¯"""
    # è·å–å½“å‰æ¶ˆæ¯
    messages = state["messages"]
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å¯¹è¯æ—¶æ·»åŠ ï¼‰
    has_system_message = any(isinstance(msg, SystemMessage) for msg in messages)
    
    if not has_system_message:
        # åœ¨æ¶ˆæ¯åˆ—è¡¨å¼€å¤´æ’å…¥ç³»ç»Ÿæç¤ºè¯
        system_message = SystemMessage(content=SYSTEM_PROMPT)
        messages_with_system = [system_message] + messages
    else:
        messages_with_system = messages
    
    # è°ƒç”¨LLM
    response = await llm_with_tools.ainvoke(messages_with_system)
    
    return {"messages": [response]}

# Build graph
graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)

# Create async tool node
async def async_tool_node(state: State):
    """å¼‚æ­¥å·¥å…·èŠ‚ç‚¹"""
    tool_node = ToolNode(tools=tools)
    return await tool_node.ainvoke(state)

graph_builder.add_node("tools", async_tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

# Create memory saver
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()

# Build the graph
graph = graph_builder.compile(checkpointer=memory)
# Configuration for the memory saver and recursion limit
config = {"configurable": {"thread_id": "1"},
          "recursion_limit": 100
          }

async def async_stream_graph_updates(user_input: str):
    """å¼‚æ­¥å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶æ˜¾ç¤ºç»“æœ"""
    tool_called = False
    
    async for event in graph.astream(
        {"messages": [{"role": "user", "content": user_input}]}, 
        config):
        
        for node_name, value in event.items():
            if node_name == "tools":
                # å¤„ç†å·¥å…·è°ƒç”¨
                tool_called = True
                last_message = value["messages"][-1]
                
                # æ‰“å°å·¥å…·å
                if hasattr(last_message, 'name'):
                    print(f"\n >>>>> ğŸ”§ å·¥å…·è°ƒç”¨: {last_message.name}")
                else:
                    print(f"\n >>>>> ğŸ”§ å·¥å…·è°ƒç”¨")

                # æ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ
                if hasattr(last_message, 'content') and last_message.content:
                    print("\n >>>>> ğŸ”§ å·¥å…·æ‰§è¡Œç»“æœ:")
                    print(f"   {last_message.content}")
                    print("â”€" * 50)
                    
            elif node_name == "chatbot":
                # å¤„ç†åŠ©æ‰‹å›å¤
                last_message = value["messages"][-1]
                
                # æ£€æŸ¥æ˜¯å¦è¦è°ƒç”¨å·¥å…·
                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                    print(last_message.tool_calls)
                    continue  # å¦‚æœè¦è°ƒç”¨å·¥å…·ï¼Œå…ˆä¸æ˜¾ç¤ºå›å¤
                
                # æ˜¾ç¤ºåŠ©æ‰‹çš„æœ€ç»ˆå›å¤
                if hasattr(last_message, 'content') and last_message.content:
                    if tool_called:
                        print("\nâœ¨ åŠ©æ‰‹: ")
                    else:
                        print("\nâœ¨ åŠ©æ‰‹: ")
                    
                    # ä½¿ç”¨æ‰“å­—æœºæ•ˆæœæ˜¾ç¤ºå›å¤
                    typewriter_print(last_message.content, delay=0.01)

def stream_graph_updates(user_input: str):
    """åŒæ­¥åŒ…è£…å™¨"""
    asyncio.run(async_stream_graph_updates(user_input))

def main():
    """ä¸»ç¨‹åº"""
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print_welcome()
    
    # å¯¹è¯å¾ªç¯
    while True:
        try:
            print_section_divider()
            user_input = input("ğŸ˜Š ç”¨æˆ·: ")
            
            if user_input.lower() in ["quit", "exit", "q", "bye"]:
                print("\nâœ¨ åŠ©æ‰‹: ğŸ‘‹ å†è§ï¼")
                break
                
            if user_input.strip():  # ç¡®ä¿è¾“å…¥ä¸ä¸ºç©º
                stream_graph_updates(user_input)
            else:
                print("âš ï¸  è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜æˆ–æŒ‡ä»¤")
                
        except KeyboardInterrupt:
            print("\nâœ¨ åŠ©æ‰‹: ğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except EOFError:
            print("\nâœ¨ åŠ©æ‰‹: ğŸ‘‹ è¾“å…¥ç»“æŸï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("ğŸ”„ è¯·é‡è¯•æˆ–è¾“å…¥å…¶ä»–æŒ‡ä»¤")

if __name__ == "__main__":
    main()
